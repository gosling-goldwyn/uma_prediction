{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Uma Prediction Model Training (Colab Version)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pandas scikit-learn tensorflow lightgbm datasets huggingface_hub python-dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "import sys\n",
                "import os\n",
                "\n",
                "from datasets import load_dataset\n",
                "from huggingface_hub import HfApi, create_repo\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Google Colab環境でのパス設定\n",
                "# リポジトリをクローンした場合のパスを想定\n",
                "if 'google.colab' in sys.modules:\n",
                "    # Colab環境の場合、カレントディレクトリをリポジトリのルートに移動\n",
                "    # これは、スクリプトがリポジトリのサブディレクトリにある場合に必要\n",
                "    # 例: /content/uma_prediction/scripts/model_training\n",
                "    # os.chdir('/content/uma_prediction') # 必要に応じて調整\n",
                "    print(\"Running in Google Colab environment.\")\n",
                "    # Colabで.envファイルを読み込むための設定\n",
                "    # from google.colab import userdata\n",
                "    # os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN') # Colab SecretsからHF_TOKENを読み込む場合\n",
                "    # または、手動でHF_TOKENを設定\n",
                "    # os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN_HERE\"\n",
                "    \n",
                "    # scripts/data_preprocessing/lgbm_categorical_processor.py をColabで利用可能にするためのパス追加\n",
                "    # リポジリのルートディレクトリをsys.pathに追加\n",
                "    # 例: /content/uma_prediction\n",
                "    project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..'))\n",
                "    if project_root not in sys.path:\n",
                "        sys.path.append(project_root)\n",
                "    print(f\"Added {project_root} to sys.path\")\n",
                "\n",
                "# training_utils.py から関数をインポート\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../..'))\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "sys.path.append(\".\")\n",
                "from scripts.model_training.training_utils import (\n",
                "    get_model_path,\n",
                "    update_training_status,\n",
                "    preprocess_data,\n",
                "    train_model,\n",
                ")\n",
                "\n",
                "# .envファイルから環境変数を読み込む\n",
                "load_dotenv()\n",
                "\n",
                "# Hugging Faceリポジトリの設定\n",
                "HF_MODEL_REPO_ID = os.getenv(\"REPO_ID\")\n",
                "hf_token = os.getenv(\"HF_TOKEN\")\n",
                "\n",
                "if hf_token is None:\n",
                "    print(\"Hugging Face token (HF_TOKEN) not found in environment variables or .env file.\")\n",
                "    print(\"Please ensure .env file exists and contains HF_TOKEN, or set it as an environment variable.\")\n",
                "    sys.exit()\n",
                "\n",
                "hf_api = HfApi(token=hf_token)\n",
                "\n",
                "# --- Model Paths ---\n",
                "DATASET_REPO_ID = os.getenv(\"DATASET_REPO_ID\")\n",
                "\n",
                "def load_data():\n",
                "    try:\n",
                "        # Hugging Face Datasetsからデータを読み込む\n",
                "        dataset = load_dataset(DATASET_REPO_ID, split=\"train\")\n",
                "        # Pandas DataFrameに変換\n",
                "        df = dataset.to_pandas()\n",
                "        print(f\"Successfully loaded data from Hugging Face Dataset: {DATASET_REPO_ID}\")\n",
                "        return df\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading data from Hugging Face Dataset: {e}\")\n",
                "        return pd.DataFrame()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create model repository on Hugging Face if it doesn't exist\n",
                "try:\n",
                "    create_repo(repo_id=HF_MODEL_REPO_ID, repo_type=\"model\", exist_ok=True, token=hf_token)\n",
                "    print(f\"Model repository '{HF_MODEL_REPO_ID}' created or already exists.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error creating/checking model repository: {e}\")\n",
                "    sys.exit()\n",
                "\n",
                "update_training_status(\n",
                "    {\"status\": \"running\", \"message\": \"Starting training process...\"}\n",
                ")\n",
                "df = load_data()\n",
                "if df.empty:\n",
                "    update_training_status({\"status\": \"error\", \"message\": \"No data loaded.\"})\n",
                "    sys.exit()\n",
                "\n",
                "for target_mode in [\"default\", \"top3\"]:\n",
                "    # With horse info\n",
                "    X_rf, y_rf, target_maps_rf = preprocess_data(\n",
                "        df.copy(), model_type=\"rf\", target_mode=target_mode\n",
                "    )\n",
                "    train_model(\"rf\", X_rf, y_rf, target_mode, hf_api=hf_api, hf_token=hf_token, hf_model_repo_id=HF_MODEL_REPO_ID, target_maps=target_maps_rf)\n",
                "    X_lgbm, y_lgbm, cats_lgbm_with_categories = preprocess_data( # 変数名を変更\n",
                "        df.copy(), model_type=\"lgbm\", target_mode=target_mode\n",
                "    )\n",
                "    train_model(\"lgbm\", X_lgbm, y_lgbm, target_mode,\n",
                "                hf_api=hf_api, hf_token=hf_token, hf_model_repo_id=HF_MODEL_REPO_ID,\n",
                "                categorical_features=[col for col in cats_lgbm_with_categories.keys()], # LGBMの引数にはカラム名リストを渡す\n",
                "                categorical_features_with_categories=cats_lgbm_with_categories)\n",
                "\n",
                "    # WITHOUT horse info\n",
                "    X_rf_no, y_rf_no, target_maps_rf_no = preprocess_data(\n",
                "        df.copy(), model_type=\"rf\", target_mode=target_mode, exclude_horse_info=True\n",
                "    )\n",
                "    train_model(\n",
                "        \"rf\", X_rf_no, y_rf_no, target_mode, horse_info=\"excluded\",\n",
                "        hf_api=hf_api, hf_token=hf_token, hf_model_repo_id=HF_MODEL_REPO_ID, target_maps=target_maps_rf_no\n",
                "    )\n",
                "    X_lgbm_no, y_lgbm_no, cats_lgbm_no_with_categories = preprocess_data( # 変数名を変更\n",
                "        df.copy(),\n",
                "        model_type=\"lgbm\",\n",
                "        target_mode=target_mode,\n",
                "        exclude_horse_info=True,\n",
                "    )\n",
                "    train_model(\"lgbm\", X_lgbm_no, y_lgbm_no, target_mode,\n",
                "                hf_api=hf_api, hf_token=hf_token, hf_model_repo_id=HF_MODEL_REPO_ID,\n",
                "                categorical_features=[col for col in cats_lgbm_no_with_categories.keys()], # LGBMの引数にはカラム名リストを渡す\n",
                "                categorical_features_with_categories=cats_lgbm_no_with_categories, horse_info=\"excluded\")\n",
                "\n",
                "    # CNN with categorical features\n",
                "    X_cnn, y_cnn, flat_cols, imputation_values, class_weight_dict = preprocess_data(\n",
                "        df.copy(), model_type=\"cnn\", target_mode=target_mode\n",
                "    )\n",
                "    train_model(\n",
                "        \"cnn\",\n",
                "        X_cnn,\n",
                "        y_cnn,\n",
                "        target_mode,\n",
                "        hf_api=hf_api, hf_token=hf_token, hf_model_repo_id=HF_MODEL_REPO_ID,\n",
                "        flat_features_columns=flat_cols,\n",
                "        imputation_values=imputation_values,\n",
                "        class_weight_dict=class_weight_dict,\n",
                "    )\n",
                "\n",
                "print(\"Model training finished.\")\n",
                "update_training_status(\n",
                "    {\"status\": \"completed\", \"message\": \"All models trained successfully.\"}\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "uma_prediction",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
